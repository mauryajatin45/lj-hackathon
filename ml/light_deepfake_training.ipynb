{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightweight Deepfake Detection Training\n",
    "\n",
    "Fast training setup for audio and video deepfake detection with lightweight models.\n",
    "\n",
    "## Features:\n",
    "- Lightweight CNN for audio deepfake detection\n",
    "- Lightweight CNN-LSTM for video deepfake detection\n",
    "- Fast training with reasonable accuracy\n",
    "- Dataset downloading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision torchaudio librosa opencv-python tqdm requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import librosa\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torchaudio\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import zipfile\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGGish + Logistic Regression Model for Fast Audio Deepfake Detection\n",
    "\n",
    "# Install required packages\n",
    "# !pip install torch torchaudio torchvggish scikit-learn soundfile\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import torch\n",
    "from torchvggish import vggish, vggish_input\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import joblib\n",
    "\n",
    "# 1) Load frozen VGGish (embeddings are 128-D)\n",
    "vggish_model = vggish()\n",
    "vggish_model.eval()\n",
    "\n",
    "def embed_wav(path):\n",
    "    \"\"\"Extract VGGish embeddings from audio file.\"\"\"\n",
    "    wav, sr = sf.read(path)     # mono or stereo\n",
    "    if wav.ndim > 1: \n",
    "        wav = wav.mean(axis=1)  # Convert to mono\n",
    "    # VGGish expects 16k; vggish_input handles resample+log-mel framing\n",
    "    examples_batch = vggish_input.waveform_to_examples(wav, sample_rate=sr)\n",
    "    with torch.no_grad():\n",
    "        emb = vggish_model.forward(torch.tensor(examples_batch).float())\n",
    "    return emb.numpy().mean(axis=0)  # average over segments -> (128,)\n",
    "\n",
    "# 2) Load data from CSV: \"path,label\" where label in {0,1}\n",
    "def load_csv(csv_path):\n",
    "    \"\"\"Load embeddings and labels from CSV file.\"\"\"\n",
    "    X, y = [], []\n",
    "    if os.path.exists(csv_path):\n",
    "        with open(csv_path) as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split(\",\")\n",
    "                if len(parts) >= 2:\n",
    "                    p, lbl = parts[0], parts[1]\n",
    "                    if os.path.exists(p):\n",
    "                        X.append(embed_wav(p))\n",
    "                        y.append(int(lbl))\n",
    "    return np.stack(X) if X else np.array([]), np.array(y)\n",
    "\n",
    "# 3) Train logistic regression on VGGish embeddings\n",
    "def train_vggish_model(train_csv=\"audio_train.csv\", val_csv=\"audio_val.csv\"):\n",
    "    \"\"\"Train VGGish + Logistic Regression model.\"\"\"\n",
    "    print(\"Loading training data...\")\n",
    "    Xtr, ytr = load_csv(train_csv)\n",
    "    if len(Xtr) == 0:\n",
    "        print(f\"No training data found in {train_csv}\")\n",
    "        return None\n",
    "    \n",
    "    print(\"Loading validation data...\")\n",
    "    Xva, yva = load_csv(val_csv)\n",
    "    if len(Xva) == 0:\n",
    "        print(f\"No validation data found in {val_csv}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Training on {len(Xtr)} samples, validating on {len(Xva)} samples\")\n",
    "    \n",
    "    # Fit logistic regression\n",
    "    clf = LogisticRegression(max_iter=1000, n_jobs=-1, class_weight=\"balanced\")\n",
    "    clf.fit(Xtr, ytr)\n",
    "    \n",
    "    # Evaluate\n",
    "    probs = clf.predict_proba(Xva)[:,1]\n",
    "    auc = roc_auc_score(yva, probs)\n",
    "    print(f\"Validation AUC: {auc:.4f}\")\n",
    "    \n",
    "    # Save model and normalization stats\n",
    "    joblib.dump(clf, \"vggish_linear.joblib\")\n",
    "    np.save(\"feature_mean_std.npy\", np.stack([Xtr.mean(0), Xtr.std(0)]))\n",
    "    print(\"Model saved as vggish_linear.joblib\")\n",
    "    \n",
    "    return clf\n",
    "\n",
    "# Simple Video Model (placeholder - implement based on your video dataset)\n",
    "class SimpleVideoModel:\n",
    "    \"\"\"Simple video deepfake detector using basic features.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.trained = False\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Placeholder - implement video feature extraction and training\n",
    "        print(\"Video model training not implemented yet\")\n",
    "        self.trained = True\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        # Placeholder\n",
    "        return np.random.rand(len(X), 2)\n",
    "\n",
    "class FastDeepFakeDetector:\n",
    "    \"\"\"Combined audio and video detector.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.audio_model = None\n",
    "        self.video_model = SimpleVideoModel()\n",
    "        self.feature_stats = None\n",
    "    \n",
    "    def load_audio_model(self, model_path=\"vggish_linear.joblib\", stats_path=\"feature_mean_std.npy\"):\n",
    "        \"\"\"Load trained audio model.\"\"\"\n",
    "        if os.path.exists(model_path):\n",
    "            self.audio_model = joblib.load(model_path)\n",
    "        if os.path.exists(stats_path):\n",
    "            self.feature_stats = np.load(stats_path)\n",
    "    \n",
    "    def predict_audio(self, audio_path):\n",
    "        \"\"\"Predict on single audio file.\"\"\"\n",
    "        if self.audio_model is None:\n",
    "            return 0.5\n",
    "        \n",
    "        emb = embed_wav(audio_path)\n",
    "        if self.feature_stats is not None:\n",
    "            emb = (emb - self.feature_stats[0]) / (self.feature_stats[1] + 1e-9)\n",
    "        \n",
    "        prob_fake = self.audio_model.predict_proba([emb])[0, 1]\n",
    "        return prob_fake\n",
    "    \n",
    "    def predict(self, audio_path=None, video_path=None):\n",
    "        \"\"\"Combined prediction.\"\"\"\n",
    "        scores = []\n",
    "        \n",
    "        if audio_path:\n",
    "            audio_score = self.predict_audio(audio_path)\n",
    "            scores.append(audio_score)\n",
    "        \n",
    "        if video_path:\n",
    "            # Placeholder for video prediction\n",
    "            video_score = 0.5\n",
    "            scores.append(video_score)\n",
    "        \n",
    "        if scores:\n",
    "            final_score = np.mean(scores)\n",
    "        else:\n",
    "            final_score = 0.5\n",
    "        \n",
    "        return {\n",
    "            \"deepfake_score\": final_score,\n",
    "            \"prediction\": \"fake\" if final_score > 0.5 else \"real\",\n",
    "            \"confidence\": abs(final_score - 0.5) * 2\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Download Functions\n",
    "\n",
    "def download_file(url, dest_path):\n",
    "    \"\"\"Download file with progress bar.\"\"\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    \n",
    "    with open(dest_path, 'wb') as file, tqdm(\n",
    "        desc=os.path.basename(dest_path),\n",
    "        total=total_size,\n",
    "        unit='iB',\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as bar:\n",
    "        for data in response.iter_content(chunk_size=1024):\n",
    "            size = file.write(data)\n",
    "            bar.update(size)\n",
    "\n",
    "def extract_archive(archive_path, extract_to):\n",
    "    \"\"\"Extract zip or tar.gz archives.\"\"\"\n",
    "    if archive_path.endswith('.zip'):\n",
    "        with zipfile.ZipFile(archive_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "    elif archive_path.endswith(('.tar.gz', '.tgz')):\n",
    "        with tarfile.open(archive_path, 'r:gz') as tar_ref:\n",
    "            tar_ref.extractall(extract_to)\n",
    "\n",
    "def download_audio_dataset():\n",
    "    \"\"\"Download LA dataset for audio deepfake detection.\"\"\"\n",
    "    print(\"Downloading LA audio dataset...\")\n",
    "    url = \"https://datashare.ed.ac.uk/bitstream/handle/10283/3336/LA.zip?sequence=3&isAllowed=y\"\n",
    "    zip_path = \"LA.zip\"\n",
    "    extract_to = \"data/audio\"\n",
    "    \n",
    "    os.makedirs(extract_to, exist_ok=True)\n",
    "    \n",
    "    if not os.path.exists(zip_path):\n",
    "        download_file(url, zip_path)\n",
    "    \n",
    "    if not os.path.exists(os.path.join(extract_to, \"LA\")):\n",
    "        print(\"Extracting LA dataset...\")\n",
    "        extract_archive(zip_path, extract_to)\n",
    "    \n",
    "    print(\"LA audio dataset ready.\")\n",
    "\n",
    "def download_video_dataset():\n",
    "    \"\"\"Download FF++ dataset subset for video deepfake detection.\"\"\"\n",
    "    print(\"Downloading FF++ video dataset subset...\")\n",
    "    # Using a smaller subset for faster training\n",
    "    url = \"https://github.com/ondyari/FaceForensics/releases/download/v1.0/faceforensics_data.zip\"\n",
    "    zip_path = \"faceforensics_data.zip\"\n",
    "    extract_to = \"data/video\"\n",
    "    \n",
    "    os.makedirs(extract_to, exist_ok=True)\n",
    "    \n",
    "    if not os.path.exists(zip_path):\n",
    "        download_file(url, zip_path)\n",
    "    \n",
    "    if not os.path.exists(os.path.join(extract_to, \"faceforensics_data\")):\n",
    "        print(\"Extracting FF++ dataset...\")\n",
    "        extract_archive(zip_path, extract_to)\n",
    "    \n",
    "    print(\"FF++ video dataset ready.\")\n",
    "\n",
    "# Download datasets\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "download_audio_dataset()\n",
    "download_video_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing and Dataset Classes\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_paths, labels, clip_seconds=3):\n",
    "        self.audio_paths = audio_paths\n",
    "        self.labels = labels\n",
    "        self.clip_seconds = clip_seconds\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Load audio at 16kHz\n",
    "        y, sr = librosa.load(audio_path, sr=16000, mono=True)\n",
    "        \n",
    "        # Trim/pad to fixed length\n",
    "        target_length = 16000 * self.clip_seconds\n",
    "        if len(y) > target_length:\n",
    "            y = y[:target_length]\n",
    "        else:\n",
    "            y = np.pad(y, (0, target_length - len(y)))\n",
    "        \n",
    "        return torch.from_numpy(y).float(), torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_paths, labels, num_frames=8, frame_size=(224, 224)):\n",
    "        self.video_paths = video_paths\n",
    "        self.labels = labels\n",
    "        self.num_frames = num_frames\n",
    "        self.frame_size = frame_size\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(frame_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        # Sample frames evenly\n",
    "        frame_indices = np.linspace(0, total_frames-1, self.num_frames, dtype=int)\n",
    "        \n",
    "        for i in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = Image.fromarray(frame)\n",
    "                frame = self.transform(frame)\n",
    "                frames.append(frame)\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        # Pad if not enough frames\n",
    "        while len(frames) < self.num_frames:\n",
    "            frames.append(torch.zeros_like(frames[0]))\n",
    "        \n",
    "        # Stack: [frames, C, H, W]\n",
    "        video_tensor = torch.stack(frames)\n",
    "        \n",
    "        return video_tensor, torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "# Generate sample data paths (you'll need to implement proper data loading)\n",
    "def get_sample_data():\n",
    "    # Placeholder - implement based on your dataset structure\n",
    "    audio_paths = []  # List of audio file paths\n",
    "    audio_labels = []  # Corresponding labels (0=real, 1=fake)\n",
    "    video_paths = []  # List of video file paths  \n",
    "    video_labels = []  # Corresponding labels\n",
    "    \n",
    "    return audio_paths, audio_labels, video_paths, video_labels\n",
    "\n",
    "# Create data loaders\n",
    "audio_paths, audio_labels, video_paths, video_labels = get_sample_data()\n",
    "\n",
    "if audio_paths:\n",
    "    audio_dataset = AudioDataset(audio_paths, audio_labels)\n",
    "    audio_loader = DataLoader(audio_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "if video_paths:\n",
    "    video_dataset = VideoDataset(video_paths, video_labels)\n",
    "    video_loader = DataLoader(video_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Dataset CSVs\n",
    "\n",
    "# Run the CSV generation script\n",
    "!python generate_dataset_csvs.py\n",
    "\n",
    "# Train VGGish + Logistic Regression Model\n",
    "\n",
    "# Install torchvggish and soundfile if not already installed\n",
    "# !pip install torchvggish soundfile\n",
    "\n",
    "audio_model = train_vggish_model(\"audio_train.csv\", \"audio_val.csv\")\n",
    "\n",
    "# Initialize multimodal detector\n",
    "detector = FastDeepFakeDetector()\n",
    "detector.load_audio_model()\n",
    "\n",
    "print(\"Training complete! VGGish + Logistic Regression model ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference Example\n",
    "\n",
    "def predict_deepfake(audio_path=None, video_path=None, model_path='light_deepfake_detector.pth'):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model = LightDeepFakeDetector()\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    audio_tensor = None\n",
    "    video_tensor = None\n",
    "    \n",
    "    if audio_path:\n",
    "        # Load and preprocess audio\n",
    "        y, sr = librosa.load(audio_path, sr=16000, mono=True)\n",
    "        target_length = 16000 * 3  # 3 seconds\n",
    "        if len(y) > target_length:\n",
    "            y = y[:target_length]\n",
    "        else:\n",
    "            y = np.pad(y, (0, target_length - len(y)))\n",
    "        audio_tensor = torch.from_numpy(y).float().unsqueeze(0).to(device)\n",
    "    \n",
    "    if video_path:\n",
    "        # Load and preprocess video\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        for i in range(8):  # 8 frames\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = Image.fromarray(frame)\n",
    "                frame = transform(frame)\n",
    "                frames.append(frame)\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        while len(frames) < 8:\n",
    "            frames.append(torch.zeros_like(frames[0]))\n",
    "        \n",
    "        video_tensor = torch.stack(frames).unsqueeze(0).to(device)  # [1, 8, C, H, W]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(audio_input=audio_tensor, video_input=video_tensor)\n",
    "        score = torch.sigmoid(outputs.squeeze()).item()\n",
    "        prediction = \"fake\" if score > 0.5 else \"real\"\n",
    "    \n",
    "    return {\n",
    "        \"deepfake_score\": score,\n",
    "        \"prediction\": prediction,\n",
    "        \"confidence\": abs(score - 0.5) * 2\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "# result = predict_deepfake(audio_path=\"path/to/audio.wav\", video_path=\"path/to/video.mp4\")\n",
    "# print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
