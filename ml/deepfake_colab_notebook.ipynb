{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepfake Detection Model Training\n",
    "\n",
    "This notebook trains a deepfake detection model using audio and video modalities with fusion capabilities.\n",
    "\n",
    "## Features:\n",
    "- Audio branch using Wav2Vec2\n",
    "- Video branch using Timesformer\n",
    "- Fusion model for multi-modal detection\n",
    "- LoRA adapters for efficient fine-tuning\n",
    "- Automatic dataset download and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision torchaudio transformers datasets kagglehub scikit-learn peft librosa opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for saving models\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository or upload files\n",
    "!git clone https://github.com/your-repo/lj-hackathon.git\n",
    "%cd lj-hackathon/ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import kagglehub\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import torchaudio\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import librosa\n",
    "from transformers import Wav2Vec2Model, TimesformerModel\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "class AudioBranch(nn.Module):\n",
    "    def __init__(self, lora_config=None):\n",
    "        super().__init__()\n",
    "        self.backbone = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "        if lora_config:\n",
    "            self.backbone = get_peft_model(self.backbone, lora_config)\n",
    "        self.classifier = nn.Linear(self.backbone.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.backbone(x)\n",
    "        return self.classifier(outputs.last_hidden_state.mean(dim=1))\n",
    "\n",
    "class VideoBranch(nn.Module):\n",
    "    def __init__(self, lora_config=None):\n",
    "        super().__init__()\n",
    "        self.backbone = TimesformerModel.from_pretrained(\"facebook/timesformer-base\")\n",
    "        if lora_config:\n",
    "            self.backbone = get_peft_model(self.backbone, lora_config)\n",
    "        self.classifier = nn.Linear(self.backbone.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.backbone(x)\n",
    "        return self.classifier(outputs.last_hidden_state[:, 0])\n",
    "\n",
    "class DeepFakeDetector(nn.Module):\n",
    "    def __init__(self, use_lora=True):\n",
    "        super().__init__()\n",
    "        lora_config = LoraConfig(r=16, lora_alpha=32) if use_lora else None\n",
    "        self.audio_branch = AudioBranch(lora_config)\n",
    "        self.video_branch = VideoBranch(lora_config)\n",
    "\n",
    "    def forward(self, audio_input=None, video_input=None):\n",
    "        if audio_input is not None and video_input is not None:\n",
    "            audio_logits = self.audio_branch(audio_input)\n",
    "            video_logits = self.video_branch(video_input)\n",
    "            return (audio_logits + video_logits) / 2\n",
    "        elif audio_input is not None:\n",
    "            return self.audio_branch(audio_input)\n",
    "        else:\n",
    "            return self.video_branch(video_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing functions\n",
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, audio_paths, labels, transform=None):\n",
    "        self.audio_paths = audio_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Load audio\n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "\n",
    "        # Convert to mel-spectrogram\n",
    "        mel_spec = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=1024,\n",
    "            hop_length=512,\n",
    "            n_mels=128\n",
    "        )(waveform)\n",
    "\n",
    "        # Convert to log scale\n",
    "        mel_spec = torchaudio.transforms.AmplitudeToDB()(mel_spec)\n",
    "\n",
    "        # Normalize\n",
    "        mel_spec = (mel_spec - mel_spec.mean()) / (mel_spec.std() + 1e-9)\n",
    "\n",
    "        return {\n",
    "            'input_values': mel_spec.squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "class VideoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, video_paths, labels, transform=None, num_frames=16):\n",
    "        self.video_paths = video_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.num_frames = num_frames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Load video\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        # Sample frames evenly\n",
    "        frame_indices = np.linspace(0, total_frames-1, self.num_frames, dtype=int)\n",
    "\n",
    "        for i in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = Image.fromarray(frame)\n",
    "                frames.append(frame)\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        # If not enough frames, duplicate last frame\n",
    "        while len(frames) < self.num_frames:\n",
    "            frames.append(frames[-1] if frames else Image.new('RGB', (224, 224)))\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            frames = [self.transform(frame) for frame in frames]\n",
    "\n",
    "        # Stack frames\n",
    "        video_tensor = torch.stack(frames, dim=1)  # [C, T, H, W]\n",
    "\n",
    "        return {\n",
    "            'pixel_values': video_tensor,\n",
    "            'labels': torch.tensor(label, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download datasets\n",
    "def download_datasets(data_dir):\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    # ASVspoof 2019\n",
    "    try:\n",
    "        asv_path = kagglehub.dataset_download(\"asvspoof/asv-spoof-2019-dataset\")\n",
    "        print(f\"ASVspoof downloaded to: {asv_path}\")\n",
    "    except:\n",
    "        print(\"ASVspoof download failed - please download manually\")\n",
    "\n",
    "    # For DFDC, you may need to download manually due to size\n",
    "    print(\"DFDC dataset is large - please download manually from https://www.kaggle.com/c/deepfake-detection-challenge\")\n",
    "\n",
    "# Create data loaders\n",
    "data_dir = '/content/data'\n",
    "download_datasets(data_dir)\n",
    "\n",
    "# Note: You'll need to implement load_asvspoof2019 and load_dfdc functions\n",
    "# For demo purposes, we'll assume you have the data ready\n",
    "print(\"Please ensure datasets are downloaded and implement data loading functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training functions\n",
    "def train_audio_model(model, train_loader, val_loader, epochs=10, save_path='/content/drive/MyDrive/models/audio_model.pth'):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "    best_auc = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            augmented_input = augment_audio(batch['input_values'])\n",
    "            outputs = model.audio_branch(augmented_input)\n",
    "            loss = criterion(outputs.squeeze(), batch['labels'])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                outputs = model.audio_branch(batch['input_values'])\n",
    "                preds = torch.sigmoid(outputs.squeeze()).cpu().numpy()\n",
    "                val_preds.extend(preds)\n",
    "                val_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "        val_auc = roc_auc_score(val_labels, val_preds)\n",
    "        val_acc = accuracy_score(val_labels, np.round(val_preds))\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Val AUC: {val_auc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        if val_auc > best_auc:\n",
    "            best_auc = val_auc\n",
    "            torch.save(model.audio_branch.state_dict(), save_path)\n",
    "            print(f\"Saved best audio model with AUC: {best_auc:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def augment_audio(mel_spec):\n",
    "    if torch.rand(1) < 0.5:\n",
    "        noise = torch.randn_like(mel_spec) * 0.1\n",
    "        mel_spec = mel_spec + noise\n",
    "    return mel_spec\n",
    "\n",
    "# Similar functions for video and fusion training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train model\n",
    "model = DeepFakeDetector(use_lora=True)\n",
    "\n",
    "# Assuming you have data loaders ready\n",
    "# audio_train_loader, audio_val_loader = get_audio_loaders(data_dir)\n",
    "# video_train_loader, video_val_loader = get_video_loaders(data_dir)\n",
    "\n",
    "# Train audio model\n",
    "# model = train_audio_model(model, audio_train_loader, audio_val_loader)\n",
    "\n",
    "print(\"Model initialized. Please implement data loading and training calls.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "model_save_path = '/content/drive/MyDrive/models/deepfake_detector.pth'\n",
    "os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference example\n",
    "def predict_deepfake(audio_path=None, video_path=None, model_path='/content/drive/MyDrive/models/deepfake_detector.pth'):\n",
    "    model = DeepFakeDetector(use_lora=True)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    # Process inputs (implement preprocessing)\n",
    "    # ...\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if audio_path and video_path:\n",
    "            # Fusion prediction\n",
    "            outputs = model(audio_input=audio_tensor, video_input=video_tensor)\n",
    "        elif audio_path:\n",
    "            # Audio-only prediction\n",
    "            outputs = model(audio_input=audio_tensor)\n",
    "        else:\n",
    "            # Video-only prediction\n",
    "            outputs = model(video_input=video_tensor)\n",
    "\n",
    "        score = torch.sigmoid(outputs.squeeze()).item()\n",
    "        prediction = \"synthetic\" if score > 0.5 else \"real\"\n",
    "\n",
    "    return {\n",
    "        \"deepfake_score\": score,\n",
    "        \"prediction\": prediction,\n",
    "        \"explanation\": [\"spectrogram anomalies\", \"face warp detected\"]  # Implement actual explanations\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "# result = predict_deepfake(audio_path=\"path/to/audio.wav\", video_path=\"path/to/video.mp4\")\n",
    "# print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
